import numpy as np
import matplotlib.pyplot as plt

# Rosenbrock function definition
def f(x, y):
    return (1 - x)**2 + 100 * (y - x**2)**2

# Gradient definitions
def dfx(x, y):
    return -2 * (1 - x) - 4 * 100 * x * (y - x**2)

def dfy(x, y):
    return 2 * 100 * (y - x**2)

# Grid for visualization
x = np.arange(-2, 2, 0.01)
y = np.arange(-1, 3, 0.01)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# Initial position
current_pos = np.array([-1.2, 1])  # Common starting point for Rosenbrock function

# Line search function
def line_search(cur_pos, alpha, gradient, max_iter=100000):
    rho = 0.9
    c = 1e-4
    for i in range(max_iter):
        new_pos = cur_pos - alpha * gradient
        if f(new_pos[0], new_pos[1]) <= f(cur_pos[0], cur_pos[1]) - c * alpha * np.dot(gradient, gradient):
            break
        alpha *= rho
    return alpha

# Perform gradient descent with line search
points = [current_pos.tolist()]  # To store the points during the iterations
for i in range(100000):
    gradient = np.array([dfx(current_pos[0], current_pos[1]), dfy(current_pos[0], current_pos[1])])
    if np.linalg.norm(gradient) < 1e-6:  # Convergence check
      print("Gradient descent converged after", i+1, "iterations.")
      print("Final position:", current_pos)
      break
    alpha = 0.1  # Smaller initial step size
    step = line_search(current_pos, alpha, gradient)
    x_new = current_pos[0] - step * gradient[0]
    y_new = current_pos[1] - step * gradient[1]
    current_pos = np.array([x_new, y_new])
    points.append(current_pos.tolist())

# Convert points to numpy array for plotting
points = np.array(points)

# Visualization
plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 50))
plt.plot(points[:, 0], points[:, 1], 'r.-')  # Path of gradient descent
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent Path with Line Search on Rosenbrock Function')
plt.show()
